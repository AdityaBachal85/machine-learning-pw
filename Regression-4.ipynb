{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c63370-f073-4964-81b3-e0f4077337b4",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\r\n",
    "**Lasso Regression (Least Absolute Shrinkage and Selection Operator)** is a type of linear regression that includes an L1 regularization term in its cost function. The L1 penalty shrinks some coefficients to zero, effectively performing feature selection by excluding less important variables from the model. \r\n",
    "\r\n",
    "- **Difference**: \r\n",
    "  - In **Linear Regression**, all features contribute to the prediction, but in **Lasso Regression**, the L1 penalty can eliminate irrelevant features by shrinking their coefficients to zero. \r\n",
    "  - It differs from **Ridge Regression** (L2 regularization) which shrinks coefficients but does not set any to zero.\r\n",
    "\r\n",
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?\r\n",
    "The main advantage is its ability to **perform automatic feature selection** by forcing the coefficients of less important features to zero. This simplifies the model and helps in dealing with high-dimensional datasets where irrelevant features may introduce noise or reduce model interpretability.\r\n",
    "\r\n",
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?\r\n",
    "- **Non-zero coefficients**: These are the features that the model considers important for prediction.\r\n",
    "- **Zero coefficients**: Features with zero coefficients are deemed irrelevant, meaning Lasso has excluded them from the model.\r\n",
    "  \r\n",
    "In general, the magnitude of non-zero coefficients reflects the contribution of each feature to the response variable, similar to linear regression.\r\n",
    "\r\n",
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\r\n",
    "The key tuning parameter in Lasso Regression is the **regularization parameter (lambda or α)**:\r\n",
    "- **High lambda**: Increases the penalty on the magnitude of coefficients, resulting in more coefficients shrinking to zero (more feature exclusion). This may reduce overfitting but can lead to underfitting if lambda is too high.\r\n",
    "- **Low lambda**: Reduces the strength of regularization, keeping most features in the model, similar to standard linear regression. This can lead to overfitting if lambda is too low.\r\n",
    "\r\n",
    "You can tune lambda via methods like **cross-validation** to find the best trade-off between model complexity and performance.\r\n",
    "\r\n",
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\r\n",
    "Lasso Regression is inherently a **linear model**, but it can handle non-linear problems if you first **transform your features**. Some common methods include:\r\n",
    "- **Polynomial Features**: Creating polynomial combinations of the original features to capture non-linear relationships.\r\n",
    "- **Kernel Trick**: Applying kernel methods that implicitly map data into a higher-dimensional space.\r\n",
    "  \r\n",
    "However, Lasso itself does not support non-linearity without such transformations.\r\n",
    "\r\n",
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?\r\n",
    "- **Ridge Regression** uses L2 regularization, which penalizes the sum of the squares of the coefficients. It tends to shrink all coefficients but doesn’t reduce any to exactly zero.\r\n",
    "- **Lasso Regression** uses L1 regularization, which penalizes the sum of the absolute values of the coefficients, resulting in some coefficients being reduced to exactly zero (feature selection).\r\n",
    "  \r\n",
    "In short, **Ridge shrinks** but does not eliminate features, while **Lasso shrinks and selects** features.\r\n",
    "\r\n",
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\r\n",
    "Yes, Lasso Regression can handle **multicollinearity** because it will **select only one feature** from a group of highly correlated features by shrinking the coefficients of the other correlated features to zero. This reduces redundancy in the model, unlike linear regression, which would keep all correlated features with non-zero coefficients.\r\n",
    "\r\n",
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\r\n",
    "The optimal value of lambda is typically chosen using **cross-validation**:\r\n",
    "- **K-fold cross-validation** is the most common method. It divides the dataset into K parts, trains the model on K-1 parts, and tests it on the remaining part, repeating this process multiple times for different values of lambda.\r\n",
    "- **Grid Search** or **Randomized Search** can also be used to explore different lambda values and find the one that minimizes the validation error.\r\n",
    "\r\n",
    "The goal is to find the lambda that provides the best balance between bias and variance, leading to better generalization on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44798f3a-6b8d-4346-9dec-ea70ef97eb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
