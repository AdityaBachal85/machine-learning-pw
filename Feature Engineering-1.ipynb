{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae71178-840e-435c-ae36-ebd0db21f8e2",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a90cfe-add9-4046-8c39-19eda783027d",
   "metadata": {},
   "source": [
    "## Missing Values in Datasets\n",
    "\n",
    "Missing values are data points that are absent for a specific variable in a dataset. They can be represented in various ways, such as blank cells, null values, or special symbols like \"NA\" or \"unknown.\" \n",
    "\n",
    "### Importance of Handling Missing Values\n",
    "\n",
    "It is essential to handle missing values efficiently for several reasons:\n",
    "\n",
    "- **Reduce sample size**: Missing data can decrease the accuracy and reliability of your analysis.\n",
    "- **Introduce bias**: If missing data is not handled properly, it can bias the results of your analysis.\n",
    "- **Make it difficult to perform certain analyses**: Some statistical techniques require complete data for all variables, making them inapplicable when missing values are present. [1]\n",
    "\n",
    "### Algorithms Unaffected by Missing Values\n",
    "\n",
    "Some machine learning algorithms can handle missing values natively, such as:\n",
    "\n",
    "- **Decision Trees**: Decision trees can handle missing values by learning patterns from the available data and making predictions based on that.\n",
    "- **Random Forests**: Random forests, an ensemble of decision trees, are also robust to missing values.\n",
    "- **XGBoost**: XGBoost, a gradient boosting library, can handle missing values by learning where to send them during the tree construction process.\n",
    "- **LightGBM**: LightGBM, another gradient boosting framework, has built-in support for missing values.\n",
    "- **CatBoost**: CatBoost, a machine learning library, can automatically handle missing values without the need for imputation. [5]\n",
    "\n",
    "These algorithms can handle missing values by learning patterns from the available data and making predictions based on that, without the need for explicit imputation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e912f7e-21ed-4f21-ade2-12e59d54775f",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81efae70-47d6-4c0e-9dd1-3ed30f0984c2",
   "metadata": {},
   "source": [
    "1. Deletion Methods\n",
    "Listwise Deletion\n",
    "This method involves removing any row that contains missing values from the dataset. It is straightforward but can lead to significant information loss if many rows have missing data.\n",
    "Pairwise Deletion\n",
    "In this approach, only the missing values are excluded from the analysis, allowing for maximum data retention. However, it can lead to inconsistencies in the dataset.\n",
    "2. Imputation Methods\n",
    "Mean, Median, and Mode Imputation\n",
    "These methods replace missing values with the mean, median, or mode of the available data in the column. This is effective for small amounts of missing data but can reduce variability.\n",
    "Last Observation Carried Forward (LOCF)\n",
    "This technique replaces missing values with the last observed value. It is commonly used in time-series data but may introduce bias if trends are present.\n",
    "Next Observation Carried Backward (NOCB)\n",
    "Similar to LOCF, this method fills missing values with the next available observation.\n",
    "3. Advanced Imputation Techniques\n",
    "K-Nearest Neighbors (KNN) Imputation\n",
    "This method uses the values of the K nearest neighbors to impute missing values, providing a more informed estimate based on the local structure of the data.\n",
    "Model-Based Imputation\n",
    "In this approach, a predictive model is trained to estimate the missing values based on other features in the dataset. This can include regression models, decision trees, or more complex algorithms.\n",
    "4. Using Algorithms that Support Missing Values\n",
    "Some machine learning algorithms, such as XGBoost and certain tree-based models, can handle missing values directly without requiring imputation. This allows for a more straightforward implementation when dealing with missing data.\n",
    "5. Time-Series Specific Methods\n",
    "For time-series data, techniques like linear interpolation can be used to estimate missing values based on trends observed in surrounding data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ea3db9-2d66-4b00-acf6-210946f2844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "1  2.0  2.0\n",
      "3  4.0  4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'A': [1, 2, None, 4], 'B': [None, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Listwise deletion\n",
    "df_cleaned = df.dropna()\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2388f538-e2d5-4971-b296-bf61ad6eedf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A    B\n",
      "0  1.000000  NaN\n",
      "1  2.000000  2.0\n",
      "2  2.333333  3.0\n",
      "3  4.000000  4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'A': [1, 2, None, 4], 'B': [None, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mean imputation\n",
    "df['A'].fillna(df['A'].mean(), inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e612fe-06d1-421a-abca-971987a3b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  3.0\n",
      "1  2.0  2.0\n",
      "2  3.0  3.0\n",
      "3  4.0  4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'A': [1, 2, None, 4], 'B': [None, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "print(pd.DataFrame(df_imputed, columns=df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121faf4-51b7-4a59-a65f-6489a3f419ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
