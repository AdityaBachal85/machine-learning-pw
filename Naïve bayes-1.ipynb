{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47707c20-9bda-4571-a59b-283de14a6d9f",
   "metadata": {},
   "source": [
    "### Q1. What is Bayes' Theorem?\n",
    "\n",
    "Bayes' Theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis based on new evidence. It provides a mathematical way to revise existing predictions or beliefs (probabilities) in light of new data.\n",
    "\n",
    "In essence, it connects **prior probability** (initial belief) with **posterior probability** (revised belief) after observing new evidence, considering the likelihood of the evidence given the hypothesis.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. What is the formula for Bayes' Theorem?\n",
    "\n",
    "The formula for Bayes' Theorem is:\n",
    "\n",
    "\\[\n",
    "P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(P(A | B)\\) is the **posterior probability** (the probability of event A happening given event B has occurred).\n",
    "- \\(P(B | A)\\) is the **likelihood** (the probability of event B happening given event A is true).\n",
    "- \\(P(A)\\) is the **prior probability** of event A (the initial belief about A before seeing B).\n",
    "- \\(P(B)\\) is the **marginal probability** (the total probability of event B happening under all hypotheses).\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. How is Bayes' Theorem used in practice?\n",
    "\n",
    "Bayes' Theorem is widely used in various fields to update predictions or beliefs in the face of new evidence. Some practical applications include:\n",
    "- **Medical Diagnosis**: Updating the probability of a disease based on test results.\n",
    "- **Spam Filtering**: Classifying an email as spam or not spam based on word frequencies.\n",
    "- **Machine Learning**: Used in algorithms like Naive Bayes classifiers for classification tasks.\n",
    "- **Weather Forecasting**: Revising predictions based on new meteorological data.\n",
    "- **Financial Risk Assessment**: Evaluating risk levels based on new market conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. What is the relationship between Bayes' Theorem and conditional probability?\n",
    "\n",
    "Bayes' Theorem is built directly upon the concept of **conditional probability**, which measures the probability of an event given that another event has occurred. Bayes' Theorem reverses conditional probabilities and allows us to compute the probability of a cause (event A) given an observed effect (event B) by relating it to the probability of observing the effect given the cause. \n",
    "\n",
    "In simpler terms, conditional probability is used to express the likelihood of something happening based on a condition, while Bayes' Theorem provides a way to update that conditional probability as new information becomes available.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "The choice of a Naive Bayes classifier depends on the nature of your input data:\n",
    "\n",
    "1. **Gaussian Naive Bayes**: Use this when the features (input variables) are continuous and are normally distributed (i.e., follow a Gaussian distribution). This is common in cases where the data approximates a bell curve.\n",
    "\n",
    "2. **Multinomial Naive Bayes**: Use this for discrete features that represent counts, like word frequencies in text classification problems (e.g., spam detection, document categorization). It works well with count data and is common in NLP tasks.\n",
    "\n",
    "3. **Bernoulli Naive Bayes**: This is ideal when the features are binary (0 or 1), representing presence or absence of a feature. It's commonly used in binary feature spaces, such as text classification where the presence/absence of a word matters more than its frequency.\n",
    "\n",
    "To choose the right type of Naive Bayes classifier, you should assess:\n",
    "- **Feature distribution**: If your features are continuous or categorical.\n",
    "- **Nature of your data**: Whether it represents counts, frequencies, or binary values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2a4d1-4244-4bd1-b073-12dc6bfb1eda",
   "metadata": {},
   "source": [
    "Q6. Assignment:\r\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\r\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\r\n",
    "each feature value for each class:\r\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\r\n",
    "A 3 3 4 4 3 3 3\r\n",
    "B 2 2 1 2 2 2 3\r\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\r\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a687985-4b49-4a3f-86aa-a94e2fa12265",
   "metadata": {},
   "source": [
    "In this assignment, we need to classify a new instance with \\(X_1 = 3\\) and \\(X_2 = 4\\) using Naive Bayes. The frequency table of each feature value for classes \\(A\\) and \\(B\\) is provided, and we assume **equal prior probabilities** for each class.\r\n",
    "\r\n",
    "### Steps for Naive Bayes Classification:\r\n",
    "1. **Calculate the likelihood** for each feature (for both classes).\r\n",
    "2. **Multiply the likelihoods** for the features for each class to get the total likelihood for each class.\r\n",
    "3. Since the prior probabilities are equal, we only need to compare the total likelihoods to determine which class is more probable.\r\n",
    "\r\n",
    "The Naive Bayes classifier formula:\r\n",
    "\\[\r\n",
    "P(C | X_1, X_2) \\propto P(X_1 | C) \\times P(X_2 | C) \\times P(C)\r\n",
    "\\]\r\n",
    "Where \\(C\\) is the class (either \\(A\\) or \\(B\\)).\r\n",
    "\r\n",
    "### Step 1: Calculate Likelihoods\r\n",
    "\r\n",
    "#### Likelihood for Class A:\r\n",
    "- From the table, we know:\r\n",
    "  - \\(P(X_1 = 3 | A) = \\frac{4}{(3 + 3 + 4)} = \\frac{4}{10}\\)\r\n",
    "  - \\(P(X_2 = 4 | A) = \\frac{3}{(4 + 3 + 3 + 3)} = \\frac{3}{13}\\)\r\n",
    "\r\n",
    "#### Likelihood for Class B:\r\n",
    "- From the table, we know:\r\n",
    "  - \\(P(X_1 = 3 | B) = \\frac{1}{(2 + 2 + 1)} = \\frac{1}{5}\\)\r\n",
    "  - \\(P(X_2 = 4 | B) = \\frac{3}{(2 + 2 + 2 + 3)} = \\frac{3}{9} = \\frac{1}{3}\\)\r\n",
    "\r\n",
    "### Step 2: Calculate Total Likelihoods for Both Classes\r\n",
    "\r\n",
    "Since the prior probabilities are equal, we just need to compute the product of the likelihoods for each class.\r\n",
    "\r\n",
    "#### For Class A:\r\n",
    "\\[\r\n",
    "P(X_1 = 3 | A) \\times P(X_2 = 4 | A) = \\frac{4}{10} \\times \\frac{3}{13} = \\frac{12}{130} = 0.0923\r\n",
    "\\]\r\n",
    "\r\n",
    "#### For Class B:\r\n",
    "\\[\r\n",
    "P(X_1 = 3 | B) \\times P(X_2 = 4 | B) = \\frac{1}{5} \\times \\frac{1}{3} = \\frac{1}{15} = 0.0667\r\n",
    "\\]\r\n",
    "\r\n",
    "### Step 3: Compare and Classify\r\n",
    "- \\(P(X_1 = 3, X_2 = 4 | A) = 0.0923\\)\r\n",
    "- \\(P(X_1 = 3, X_2 = 4 | B) = 0.0667\\)\r\n",
    "\r\n",
    "Since \\(0.0923 > 0.0667\\), the Naive Bayes classifier predicts that the new instance with \\(X_1 = 3\\) and \\(X_2 = 4\\) belongs to **Class A**.\r\n",
    "\r\n",
    "### Final Answer:\r\n",
    "The Naive Bayes classifier would predict the new instance to belong to **Class A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663bb76-2f1f-4495-8b68-85a8b9edf1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
