{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9f1c97-5f1b-4354-a51d-a0e4091f570e",
   "metadata": {},
   "source": [
    "\n",
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "**Kernel Functions**:\n",
    "- In machine learning, particularly in SVMs, kernel functions allow algorithms to operate in high-dimensional spaces without explicitly computing the coordinates of the data in that space.\n",
    "- A kernel function computes the inner product of two vectors in the feature space, providing a way to measure similarity.\n",
    "\n",
    "**Polynomial Functions**:\n",
    "- A polynomial kernel is a specific type of kernel function defined as:\n",
    "  \\[\n",
    "  K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
    "  \\]\n",
    "  where \\(c\\) is a constant, \\(d\\) is the degree of the polynomial, and \\(x_i\\) and \\(x_j\\) are the input feature vectors.\n",
    "- The polynomial kernel enables SVMs to classify data that is not linearly separable by implicitly mapping the input features into a higher-dimensional space, allowing for more complex decision boundaries.\n",
    "\n",
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "You can implement an SVM with a polynomial kernel using the `SVC` class from Scikit-learn by specifying the `kernel` parameter as `'poly'`. Hereâ€™s a simple example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM with polynomial kernel\n",
    "poly_svm = SVC(kernel='poly', degree=3, C=1.0)  # You can adjust the degree and C\n",
    "poly_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "accuracy = poly_svm.score(X_test, y_test)\n",
    "print(f\"SVM with Polynomial Kernel Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Optionally plot decision boundaries (not shown here for brevity)\n",
    "```\n",
    "\n",
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "- **Epsilon (\\( \\epsilon \\))**: In Support Vector Regression (SVR), epsilon defines a margin of tolerance where no penalty is given to errors. Specifically, it specifies a threshold where predictions are allowed to deviate from the actual values without affecting the cost function.\n",
    "- **Effect of Increasing Epsilon**:\n",
    "  - **More Tolerance**: As epsilon increases, the model allows a larger margin of error, leading to fewer support vectors. This is because more points can lie within the epsilon-insensitive zone without being counted as errors.\n",
    "  - **Less Complexity**: Fewer support vectors typically result in a simpler model, which may lead to underfitting if epsilon is set too high.\n",
    "\n",
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)?\n",
    "\n",
    "1. **Kernel Function**:\n",
    "   - The choice of kernel (linear, polynomial, RBF, etc.) determines the model's ability to capture complex relationships.\n",
    "   - **Example**: Use a linear kernel for simple, linearly separable data; use a polynomial or RBF kernel for more complex relationships.\n",
    "\n",
    "2. **C Parameter**:\n",
    "   - The \\(C\\) parameter controls the trade-off between achieving a low training error and a low testing error.\n",
    "   - A **high \\(C\\)** value encourages the model to fit the training data closely, potentially leading to overfitting.\n",
    "   - A **low \\(C\\)** value increases the margin, allowing some errors and potentially leading to underfitting.\n",
    "   - **Example**: If your model is overfitting, consider lowering \\(C\\); if it's underfitting, try increasing \\(C\\).\n",
    "\n",
    "3. **Epsilon Parameter**:\n",
    "   - As discussed, \\( \\epsilon \\) defines the margin of tolerance for SVR.\n",
    "   - Increasing epsilon allows for more errors without penalty, typically reducing the number of support vectors.\n",
    "   - **Example**: If your model is too complex and overfitting, increasing \\( \\epsilon \\) might help simplify it.\n",
    "\n",
    "4. **Gamma Parameter (for RBF kernel)**:\n",
    "   - The gamma parameter defines how far the influence of a single training example reaches. A low value means 'far' and a high value means 'close.'\n",
    "   - **High gamma** results in more complex models that can fit the training data very closely, which may lead to overfitting.\n",
    "   - **Low gamma** creates a smoother decision boundary, potentially leading to underfitting.\n",
    "   - **Example**: If you notice that your model is highly sensitive to noise in the training data, consider reducing gamma.\n",
    "\n",
    "### Summary\n",
    "In summary, understanding how these parameters work together is crucial for effectively tuning your SVR model. The goal is to find a balance between model complexity and generalization performance. Adjust these parameters based on your specific dataset characteristics and the results of your initial experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea65dee-0bb8-43c4-b4f3-c3d65d3eaca5",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L Use the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe344004-13f3-4252-a4ec-e2e29b0233bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Best parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Model saved as 'best_svc_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib  # For saving the model\n",
    "\n",
    "# Step 2: Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC(kernel='rbf', random_state=42)  # Using RBF kernel\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 6: Predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Step 7: Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 8: Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']  # Using RBF kernel for tuning\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Step 9: Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 10: Save the trained classifier to a file for future use\n",
    "joblib.dump(best_svc, 'best_svc_model.pkl')\n",
    "print(\"Model saved as 'best_svc_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2e435-043b-4b65-99ad-cd249e0e49b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
